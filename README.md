# Scraper for Snag Campsites

Personal project for scraping the recreation.gov site on a schedule.
Includes a UI for job creation created through Flask, Jinja

These instructions may or may not be kept vague on purpose.

## Setup
1. create a config.py  
  - required variables found n config.exmaple.py, requires smpt and imap config for an email account and a twilio token and Key
  - Also requires that you create scraper.db file somewhere and include in the config
3. Install the requirements from requirements.txt
2. Add the cron job for the scraper process to run at whatever frequency you like.
  run `crontab -e` and add the line  
   `* * * * * python scraper_runner.py`
   or something of that flavor  
3. Run the DB migrations folder  
  `sqlite3 scraper.db < migrations/base.sql`  
  `sqlite3 scraper.db < migrations/populate_sites.sql`
4. Serve the UI.  
  `python server.py`

## Key components
### scraper.py
This script contains the main logic for the scraping of campsite listings. It currently is abstracted so that it can work with nearly any campsite on the website though it is probably only particularly useful for extremely overbooked sites such as Yosemite and Yellowstone

### site.yaml
Includes the codes used to define a given site. You can glean these through chrome debugger tools from the campsite listing site and the urls often generated by navigating through the website

### notifications.py
Utility class for sending notifications based on the config.py

### db_helper.py
This script is shared between the scraper.py and server.py so that they can CRUD jobs to the sqlite db

### config.py
There's an example config.py contained in config.example.py
This uses twilio and smtplib to send texts and emails to notify you once a vacant site is found
If you want to use this you'll have to create a config.py in the project root with the appropriate information.
It also contains a name for a sqlite db to store scraper jobs

## General Architecture
You submit a post request from the flask served form with your name, date, length of stay and contact information to create a job
the job is logged into the db
the scraper runs periodically on a crontab creating scrapers for each job and running them serially. Once the jobs are run it will remove any jobs with dates past the current date from the db.

## UX
- User navigates to site
- User selects the general location they want to camp.  
  At first it'll only include Tuolumne Meadows and The Valley
  We do not discriminate on site size.
- They enter the required information
  - First Name, Last Name
  - Arrival Dates, separated by commas
  - Length of Stay
- Optional Information  
  Note to use that we need an email address or phone number in order to notify them of a vacancy.
  - Email Address
  - Phone Number

non-mvp features
  - On the success page e-mail them a job number and url they can go back to edit or cancel the job.  
  This should be random UUID so people can't edit or delete other peoples jobs

## TODO:
- Redo timestamps to use epoch time instead of timestamps so filterint works
- Make a init_db.sh or .py that will gracefully handle making sure the db is setup correctly with tables and site data.
